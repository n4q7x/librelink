



# LinkedIn's Dark Patterns

The term “dark patterns” was coined by UX designer Harry Brignull in 2010, when he launched [darkpatterns.org](https://www.deceptive.design/), a site meant to “name and shame” deceptive user-interface patterns. He later elaborated the idea in his article “Dark Patterns: Deception vs. Honesty in UI Design” in *A List Apart* ([original](https://alistapart.com/article/dark-patterns-deception-vs-honesty-in-ui-design/) · [archived](https://perma.cc/7XBV-LYSX)). Brignull defined a dark pattern as “a user interface that has been carefully crafted to trick users into doing things they might not otherwise do, such as buying insurance with their purchase or signing up for recurring bills” (Brignull, 2010; see also his 2023 book *Deceptive Patterns*).

Tech and design coverage in the 2010s began using “dark patterns” to describe manipulative interfaces on major platforms; human–computer interaction researchers such as Gray et al. (2018) and Mathur et al. (2019) formalized taxonomies and crawled shopping sites to quantify how widespread these practices were; and legal/policy work by Luguri & Strahilevitz, King, Narayanan and others helped establish “dark patterns” as a term of art in privacy and consumer-protection debates. Regulators and advocates then picked it up explicitly, through efforts like the U.S. Federal Trade Commission’s *Bringing Dark Patterns to Light* report, EU guidance on “deceptive design patterns,” and a public dark-pattern tip line run by the Electronic Frontier Foundation and *Consumer Reports*.

Although Brignull’s original emphasis was on deception, later work makes it clear that dark patterns are broader than simply “misleading.” They are any interface choices that systematically put the service’s interests ahead of the user’s by *steering, coercing, or exploiting* them: interfaces that hijack attention, attempt to coerce behavior, obscure intentions, or treat users as means to some other end (for example, data extraction, ad impressions, or lock-in), rather than prioritizing user autonomy and well-being. Legal and HCI scholars now routinely describe dark patterns as designs that knowingly confuse users, exploit cognitive and emotional vulnerabilities, or make it harder to enact one’s actual preferences, especially around money, privacy, and time (see Narayanan et al. 2020; Luguri & Strahilevitz 2021; Gunawan et al. 2021).

One especially important battleground is the subscription and “recurring billing” economy. Critics argue that many subscription-based services normalize dark-pattern-adjacent practices such as “forced continuity,” “negative-option billing,” and “subscription traps”: free trials that silently roll into paid plans, auto-renewal by default, and cancellation flows that are far harder and more time-consuming than signing up. Design and policy work now treat these as canonical patterns—“forced continuity,” “roach motel,” or “subscription trap”—and regulators have brought cases against companies whose subscriptions were easy to start but confusing or exhausting to cancel (for example, enforcement actions and settlements involving Amazon Prime, HelloFresh, SeaWorld, various streaming and learning platforms, and proposed “click-to-cancel” rules that would require cancellation to be as easy as sign-up). In all of these, the burden is shifted onto the user to remember to cancel and to fight their way through a maze of UI friction.

Social platforms have been the other major laboratory for dark patterns. Facebook in particular is often cited as emblematic of attention-harvesting design: infinite scroll, autoplay, engagement-optimized feeds, complex deactivation or privacy settings, and aggressive notification schemes have all been analyzed as dark patterns or closely related “sludge” by HCI and media scholars. Since then, other platforms have adopted similar engagement-hacking techniques. Quora, for instance, has been criticized for mobile pop-ups that block reading until the user is funneled into its app, as well as hard-to-find logout and notification controls; LinkedIn has been scrutinized for growth flows that accessed entire contact lists and sent repeated invitation emails (leading to a class-action settlement in *Perkins v. LinkedIn*), and for notification and unsubscribe interfaces that make it tedious to turn off unwanted prompts. While the concrete mechanisms differ—endless feeds, pop-up gates, contact-harvesting, or notification spam—the underlying strategy is shared: design the interface so that users spend more time, share more data, and say “yes” more often than they would under clear, neutral, easily reversible choices.

Building on Brignull’s original examples, later taxonomies (for example, Gray et al. 2018; Mathur et al. 2019; Narayanan et al. 2020) converge on several broad families of dark patterns:

- **Nagging** – persistent prompts or interruptions that derail the user’s current task (for example, constant pop-ups to enable notifications, rate the app, or upgrade to a paid tier).
- **Obstruction** – making certain actions (cancelling a subscription, closing an account, opting out of tracking) much harder than staying in, often via deep navigation, repeated confirmation screens, or required phone calls/chat with an agent.
- **Sneaking** – hiding or delaying key information, such as quietly adding items to a cart, revealing mandatory fees only at the last step, or turning “free trials” into paid subscriptions with only fine-print disclosure.
- **Interface interference** – manipulating layout or visual hierarchy so that the profitable or data-hungry option is visually dominant, while the privacy-preserving or cheaper option is small, low-contrast, or ambiguously worded.
- **Forced action / forced continuity** – requiring users to perform unrelated actions (like creating an account, sharing contacts, or agreeing to broad data collection) as a condition of using basic features, or silently continuing a paid service unless the user navigates a hostile cancellation flow.
- **Urgency and scarcity** – countdown timers, “only 2 left at this price,” “X people are viewing this right now,” and similar pressure tactics that push users into quicker, less reflective decisions.
- **Social proof and confirmshaming** – exaggerated or inauthentic testimonials, and opt-out choices phrased to shame the user (“No thanks, I hate saving money”) or imply social norms that steer them toward the more profitable action.

Taken together, these families make it easier to see that “dark patterns” are not just a grab-bag of annoying tricks but a strategy: using interface design and behavioral insights to create systematic asymmetries of power, information, and effort between platforms and the people who use them.








## Documentation (ie video screen grabs)

- LinkedIn forciy logs you in via Google even aftee you logged out
- LinkedIn uses hostile engagement tactics like puzzle games, news, news feed, people you may know, and tries to force you to post because it apparently increases your chances of getting a job
- LinkedIn dominates the job network space with network effects. There is no good alternative, yet it's awful. This must be rectified.
- LinkedIn does not have transparent control over notifications. Even if you turn off all notifications, they will send you more ones.



# And hopefully, holding people accountable for doing this

- who made these business decisions?
- what legal grounds are there for consequences?

# Other ways to undermine LinkedIn

- boycott
- share information about how poorly they respect user rights
- use alternativr platforms





# LinkedIn's Dark Patterns

The term “dark patterns” was coined by UX designer Harry Brignull in 2010, when he launched the site [darkpatterns.org](https://www.deceptive.design/) to catalogue deceptive interface designs. He later expanded on this concept in his article “Dark Patterns: Deception vs. Honesty in UI Design” (Brignull, 2011, A List Apart, https://alistapart.com/article/dark-patterns-deception-vs-honesty-in-ui-design/). He defined dark patterns as “tricks used in websites and apps that make you do things that you didn’t mean to, like buying or signing up for something” (Brignull, 2010, https://www.deceptive.design/). The term quickly caught hold with UX designers, regulators, and researchers, as more people began to document how major platforms and e-commerce sites used dark patterns such as misleading opt-outs, pre-checked boxes, and confusing subscription or privacy settings.

But dark patterns are arguably much broader than simply being misleading. Any aspect of user interaction design that is malicious can be considered a dark pattern: interfaces which hijack information, try to coerce behavior, a lack of transparent intentions, trying to use users for other ends, not prioritizing user well being including emotional and psychological well being, exploitative tactics to increase spending, and so on. Arguably, the entire payment model of the subscription economy is built on and nornalizes dark patterns, becuass it tries to get the user to pay kore than they consciously would choose to by not allowing them to sign ip for services unless they commit to monthlh susbcriptions, with no setting to auto turn off at a certain point.  Thus, the onus is on tje user to rememebr to turk off their subscdiptin.

Of course, the inventors of a whole cadre of modern dark patterns - perhaps the initiator of the very idea of an entire software business model based on dark patterns - was Facebook. Since then, 2 websites we have seen borrow their engagement hacking techniques have especially been Quora and LinkedIn. 



The term “dark patterns” was coined by UX designer Harry Brignull in 2010, when he launched [darkpatterns.org](https://www.deceptive.design/) (originally darkpatterns.org), a site meant to “name and shame” deceptive user-interface patterns. He later elaborated the idea in his article “Dark Patterns: Deception vs. Honesty in UI Design” in *A List Apart* ([original](https://alistapart.com/article/dark-patterns-deception-vs-honesty-in-ui-design/) · [archived](https://perma.cc/7XBV-LYSX)). He defined a dark pattern as “a user interface that has been carefully crafted to trick users into doing things they might not otherwise do.”

The term quickly caught hold among UX practitioners, journalists, researchers, and regulators. Mid-2010s tech and design coverage began using “dark patterns” to describe manipulative interfaces on major platforms; human–computer interaction researchers such as Gray et al. (2018) and Mathur et al. (2019) analyzed practitioner examples and crawled thousands of shopping sites to quantify how widespread these practices were; and legal/policy work by Luguri & Strahilevitz, King, Narayanan and others helped establish “dark patterns” as a term of art in privacy and consumer-protection debates. Regulators and advocates then picked it up explicitly, through events like the U.S. Federal Trade Commission’s *Bringing Dark Patterns to Light* workshop, EU rules on “deceptive design patterns,” and a public tip line run by the Electronic Frontier Foundation and *Consumer Reports* for people to report dark patterns they encounter.

Brignull’s original catalogue and later academic taxonomies (for example, Gray et al. 2018 and Mathur et al. 2019) broadly agree on several major families of dark patterns:

- **Nagging** – repeated prompts or interruptions that derail the user’s task (for example, constant pop-ups to enable notifications or upgrade to a paid tier).
- **Obstruction** – making certain actions (cancelling a subscription, closing an account, opting out of tracking) much harder than staying in, often via multi-step, confusing flows.
- **Sneaking** – hiding or delaying key information, such as quietly adding items to a cart, revealing mandatory fees only at the last step, or turning “free trials” into auto-renewing subscriptions.
- **Interface interference** – manipulating layout or visual hierarchy so that the profitable or data-hungry option is visually dominant, while the privacy-preserving or cheaper option is small, low-contrast, or buried.
- **Forced action** – requiring users to perform unrelated actions (creating an account, sharing contacts, agreeing to extensive data collection) as a condition of using basic features.
- **Urgency and scarcity** – countdown timers, “only 2 left at this price,” and “X people are viewing this right now,” used to pressure users into faster, less reflective decisions.
- **Social proof and confirmshaming** – exaggerated or inauthentic testimonials, and opt-out choices phrased to shame the user (“No thanks, I hate saving money”) to steer them toward the more profitable action.


## Documentation (ie video screen grabs)

- LinkedIn forciy logs you in via Google even aftee you logged out
- LinkedIn uses hostile engagement tactics like puzzle games, news, news feed, people you may know, and tries to force you to post because it apparently increases your chances of getting a job
- LinkedIn dominates the job network space with network effects. There is no good alternative, yet it's awful. This must be rectified.
- LinkedIn does not have transparent control over notifications. Even if you turn off all notifications, they will send you more ones.



# And hopefully, holding people accountable for doing this

- who made these business decisions?
- what legal grounds are there for consequences?

# Other ways to undermine LinkedIn

- boycott
- share information about how poorly they respect user rights
- use alternativr platforms

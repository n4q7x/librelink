


The term “dark patterns” was coined by UX designer Harry Brignull in 2010, when he launched [darkpatterns.org](https://www.deceptive.design/) (originally darkpatterns.org), a site meant to “name and shame” deceptive user-interface patterns. He later elaborated the idea in his article “Dark Patterns: Deception vs. Honesty in UI Design” in *A List Apart* ([original](https://alistapart.com/article/dark-patterns-deception-vs-honesty-in-ui-design/) · [archived](https://perma.cc/7XBV-LYSX)). Brignull defined a dark pattern as “a user interface that has been carefully crafted to trick users into doing things they didn’t mean to, like buying or signing up for something,” a formulation he continues to use on [deceptive.design](https://www.deceptive.design/what-are-dark-patterns) and in his 2023 book *Deceptive Patterns*.

The term quickly caught hold among UX practitioners, journalists, researchers, and regulators. Human–computer interaction and privacy scholars such as Gray et al. (“[The Dark (Patterns) Side of UX Design](https://dl.acm.org/doi/10.1145/3173574.3174108)”, 2018), Mathur et al. (“[Dark Patterns at Scale](https://arxiv.org/abs/1907.07032)”, 2019), and Narayanan et al. (“[Dark Patterns: Past, Present, and Future](https://queue.acm.org/detail.cfm?id=3400901)”, 2020) catalogued examples and showed how widespread they are, especially in e-commerce and privacy settings. Legal scholars such as Luguri and Strahilevitz (“[Shining a Light on Dark Patterns](https://academic.oup.com/jla/article/13/1/43/6180579)”, 2021) ran experiments showing that dark patterns can massively increase uptake of exploitative offers. Regulators and advocates picked up the term as well: the U.S. Federal Trade Commission held a public workshop and later issued its staff report “[Bringing Dark Patterns to Light](https://www.ftc.gov/system/files/ftc_gov/pdf/P214800%2BDark%2BPatterns%2BReport%2B9.14.2022%2B-%2BFINAL.pdf)” (2022); the FTC followed with an enforcement policy statement on subscription dark patterns ([press release](https://www.ftc.gov/news-events/news/press-releases/2021/10/ftc-ramp-enforcement-against-illegal-dark-patterns-trick-or-trap-consumers-subscriptions), 2021); and in Europe, regulators issued guidance such as the [EDPB Guidelines 03/2022 on deceptive design patterns](https://www.edpb.europa.eu/our-work-tools/our-documents/guidelines/guidelines-032022-deceptive-design-patterns-social-media_en) and restrictions in the [Digital Services Act](https://en.wikipedia.org/wiki/Digital_Services_Act) and related rules. Civil-society groups launched the “[Dark Patterns Tip Line](https://darkpatternstipline.org/)” (Consumer Reports, EFF, others) to crowdsource examples from the public.

Although Brignull’s original emphasis was on deception, later work makes it clear that dark patterns are broader than simply “misleading.” They include any interface choices that systematically put the service’s interests ahead of the user’s by steering, coercing, or exploiting them: interfaces that hijack attention or information, attempt to coerce behavior, obscure intentions, or treat users as a means to other ends (data extraction, ad impressions, lock-in) rather than prioritizing user autonomy and well-being. Narayanan et al. (2020), Luguri & Strahilevitz (2021), and others describe dark patterns as designs that knowingly confuse users, exploit cognitive and emotional vulnerabilities, or make it harder to enact one’s actual preferences, especially around money, privacy, and time.

One especially important battleground is the subscription and “recurring billing” economy. Critics argue that many subscription-based services normalize patterns like “forced continuity,” “negative-option billing,” and “subscription traps”: free trials that silently roll into paid plans, auto-renewal by default, and cancellation flows that are far harder than signing up. These have been central to major enforcement actions: for example, the FTC’s $100 million settlement with Vonage over cancellation “obstacles” and junk fees ([FTC press release](https://www.ftc.gov/news-events/news/press-releases/2022/11/ftc-action-against-vonage-results-100-million-customers-trapped-illegal-dark-patterns-junk-fees-when-trying-cancel-service)), and its lawsuit and subsequent multibillion-dollar settlement with Amazon over Prime enrollment and cancellation flows that the FTC explicitly described as “dark patterns” ([complaint & case overview](https://www.ftc.gov/legal-library/browse/cases-proceedings/2123050-amazoncom-inc-rosca-ftc-v); see also recent [settlement coverage](https://time.com/7320708/amazon-prime-ftc-lawsuit-settlement-membership-subscription-cancel-dark-patterns/)). The FTC also adopted a “click-to-cancel” rule to require cancellation to be as easy as sign-up ([rule announcement](https://www.ftc.gov/news-events/news/press-releases/2024/10/federal-trade-commission-announces-final-click-cancel-rule-making-it-easier-consumers-end-recurring)), although that rule has since been challenged and partially struck down in court ([example analysis](https://www.theregreview.org/2025/10/04/seminar-regulating-dark-patterns/)). In all of these cases, the burden is shifted onto the user to remember to cancel and to fight through friction-laden flows.

Social platforms have been another major laboratory for dark patterns. Facebook is often cited as emblematic of attention-harvesting design: infinite scroll, autoplay, engagement-optimized feeds, complex deactivation or privacy settings, and aggressive notifications are regularly analyzed as dark patterns or adjacent “sludge” in HCI and media studies (see, for example, Narayanan et al. 2020 and many case studies cited there). Other platforms followed suit. **Quora** has been repeatedly criticized for design choices such as aggressive login and app-install “walls” that block reading content in the browser, forced or default opt-ins to marketing email, and account-deletion flows with forced waiting periods (see, e.g., the [Quora entries in Brignull’s brand catalogue](https://www.deceptive.design/brands/quora), the [“Quora: Automatic Opt-In” dark pattern case](https://darkpatterns.uxp2.com/pattern/quora-automatic-opt-in/), and user workarounds documented by the community, such as [bypassing the Quora login popup](https://webapps.stackexchange.com/questions/110080/how-can-i-bypass-the-annoying-quora-login-popup)). **LinkedIn** has faced similar scrutiny: its “Add Connections” flow harvested entire email address books and repeatedly emailed contacts in ways that led to the *Perkins v. LinkedIn* class action and a roughly $13 million settlement ([case discussion and documents](https://www.deceptive.design/brands/linkedin); see also mainstream coverage like [Fast Company’s write-up](https://www.fastcompany.com/3051906/after-lawsuit-settlement-linkedins-dishonest-design-is-now-a-13-million-problem)). Commentators and designers have since pointed out further LinkedIn patterns around notifications, recommended connections, and growth prompts that fit the same engagement-hacking logic.

Building on Brignull’s early catalogue, later taxonomies (e.g., Gray et al. 2018; Mathur et al. 2019; Narayanan et al. 2020) converge on several broad families of dark patterns:

- **Nagging** – persistent prompts or interruptions that derail the user’s current task (e.g., constant pop-ups to enable notifications, rate the app, or upgrade to a paid tier).
- **Obstruction** – making certain actions (cancelling a subscription, closing an account, opting out of tracking) much harder than staying in, often via deep navigation, repeated confirmation screens, or required synchronous contact with support.
- **Sneaking** – hiding or delaying key information, such as quietly adding items to a cart, revealing mandatory fees only at the last step, or turning “free trials” into paid subscriptions with only fine-print disclosure.
- **Interface interference** – manipulating layout or visual hierarchy so that the profitable or data-hungry option is visually dominant (bright buttons, large text, default selection), while the privacy-preserving or cheaper option is small, low-contrast, or ambiguously worded.
- **Forced action / forced continuity** – requiring users to take unrelated actions (creating an account, sharing contacts, agreeing to broad data collection) as a condition of using basic features, or silently continuing a paid service unless the user navigates a hostile cancellation flow.
- **Urgency and scarcity** – countdown timers, “only 2 left at this price,” “X people are viewing this right now,” and similar pressure tactics that push users into faster, less reflective decisions.
- **Social proof and confirmshaming** – exaggerated or inauthentic testimonials, and opt-out choices phrased to shame the user (“No thanks, I hate saving money”) or imply social norms that steer them toward the more profitable action.

Taken together, these families make it clear that “dark patterns” are not just isolated tricks but an overall strategy: using interface design and behavioral insights to create systematic asymmetries of power, information, and effort between platforms and the people who use them.

Sadly, software has become a new venue for the tired cliche of the company that will do anything in the name of profit, from the tobacco industry to the gambling industry. Enormous amounts of money are made by businesses which do not inherently value the well-being of their user base. We can fight against those kinds of companies by rejecting the use of non-free software and building free software alternatives that do not normalize nalware as part of civic life.


## Documentation (ie video screen grabs)

- LinkedIn forciy logs you in via Google even aftee you logged out
- LinkedIn uses hostile engagement tactics like puzzle games, news, news feed, people you may know, and tries to force you to post because it apparently increases your chances of getting a job
- LinkedIn dominates the job network space with network effects. There is no good alternative, yet it's awful. This must be rectified.
- LinkedIn does not have transparent control over notifications. Even if you turn off all notifications, they will send you more ones.



# And hopefully, holding people accountable for doing this

- who made these business decisions?
- what legal grounds are there for consequences?

# Other ways to undermine LinkedIn

- boycott
- share information about how poorly they respect user rights
- use alternativr platforms
